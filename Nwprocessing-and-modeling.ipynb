{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39ab2ea-52b7-4d37-9d61-0841bdf34fe4",
   "metadata": {},
   "source": [
    "### Model 3: Attention-based Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d1e33-35e4-40db-a29d-fc0e593e608b",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54aee52a-e7b3-44c4-a391-a3541d1eda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95fe1e9f-c5c0-434c-8ef2-c1b1e1ed47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature + target\n",
    "X = model_data.drop(columns=['Severity'])\n",
    "y = model_data['Severity']\n",
    "\n",
    "# Drop datetime\n",
    "datetime_cols = X.select_dtypes(include=['datetime64[ns]', 'datetime64']).columns\n",
    "X = X.drop(columns=datetime_cols)\n",
    "\n",
    "# Identify numeric / categorical / boolean\n",
    "numeric = [\n",
    "    'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)',\n",
    "    'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)',\n",
    "    'Precipitation(in)', 'kde_ix', 'kde_iy',\n",
    "    'kde_1km', 'kde_density_m2', 'kde_grid_count',\n",
    "    'kde_grid_density_m2', 'kde_grid_x', 'kde_grid_y',\n",
    "    'cell1_count', 'cell1_mean_sev'\n",
    "]\n",
    "\n",
    "categorical = [\n",
    "    'Wind_Direction', 'Weather_Condition', 'Sunrise_Sunset',\n",
    "    'Weather_Clean', 'Weather_Intensity',\n",
    "    'cell_1km', 'cell_5km', 'kde_cell_kdegrid'\n",
    "]\n",
    "\n",
    "boolean = [\n",
    "    'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit',\n",
    "    'Railway', 'Roundabout', 'Station', 'Stop',\n",
    "    'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop',\n",
    "    'Weather_Windy', 'Weather_Thunder'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6804c37f-27d4-4391-adb8-ffe1b9aa37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop datetime columns\n",
    "datetime_cols = X.select_dtypes(include=['datetime64[ns]', 'datetime64']).columns\n",
    "X = X.drop(columns=datetime_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b90beb9-c583-48c4-83d3-ad452c0ebaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-cardinality: ['Weather_Condition', 'cell_1km', 'cell_5km', 'kde_cell_kdegrid']\n",
      "Low-cardinality: ['Wind_Direction', 'Sunrise_Sunset', 'Weather_Clean', 'Weather_Intensity']\n"
     ]
    }
   ],
   "source": [
    "X_encoded = X.copy()\n",
    "\n",
    "# Identify high-cardinality columns ( >50 unique values )\n",
    "high_cardinality = [col for col in categorical if X[col].nunique() > 50]\n",
    "\n",
    "# Low-cardinality columns (safe to one-hot)\n",
    "low_cardinality = [col for col in categorical if X[col].nunique() <= 50]\n",
    "\n",
    "print(\"High-cardinality:\", high_cardinality)\n",
    "print(\"Low-cardinality:\", low_cardinality)\n",
    "\n",
    "# One-hot encode only low-cardinality\n",
    "X_encoded = pd.get_dummies(X_encoded, columns=low_cardinality, drop_first=True)\n",
    "\n",
    "# Handle high-cardinality with frequency encoding\n",
    "for col in high_cardinality:\n",
    "    freqs = X[col].value_counts()\n",
    "    X_encoded[col] = X[col].map(freqs)\n",
    "\n",
    "# Ensure boolean are ints\n",
    "for col in boolean:\n",
    "    X_encoded[col] = X_encoded[col].astype(int)\n",
    "\n",
    "# Fill NaN\n",
    "X_encoded = X_encoded.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ddeed0e-a661-436a-8d0c-608f91f7b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.3, random_state=1, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "109b6482-a20f-4005-8c17-2d5f0c245c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: [1 2 3 4] [1 2 3 4]\n",
      "Mapped labels: [0 1 2] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original labels:\", np.unique(y_train), np.unique(y_test))\n",
    "\n",
    "# Map labels to 0-based indices for 3 classes\n",
    "y_train = y_train.copy()\n",
    "y_test = y_test.copy()\n",
    "\n",
    "# Example mapping: 1->0, 2->1, 3->2, 4->2 (merge Severe & Extra Severe)\n",
    "y_train[y_train == 1] = 0\n",
    "y_train[y_train == 2] = 1\n",
    "y_train[y_train == 3] = 2\n",
    "y_train[y_train == 4] = 2\n",
    "\n",
    "y_test[y_test == 1] = 0\n",
    "y_test[y_test == 2] = 1\n",
    "y_test[y_test == 3] = 2\n",
    "y_test[y_test == 4] = 2\n",
    "\n",
    "print(\"Mapped labels:\", np.unique(y_train), np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7b2c3b9-efca-4d66-9159-1ceed4c6b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca1e4027-0f1c-4868-b369-7994aa848249",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "feature_names = X_train.columns.tolist()  # store for attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22041464-c63c-4096-bce4-0b257ab15f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FeatureAttentionLayer defined!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class FeatureAttentionLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, attention_dim=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Attention scoring network\n",
    "        self.attention_weights = self.add_weight(\n",
    "            name='attention_weights',\n",
    "            shape=(input_shape[-1], self.attention_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.attention_bias = self.add_weight(\n",
    "            name='attention_bias',\n",
    "            shape=(self.attention_dim,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_weights = self.add_weight(\n",
    "            name='output_weights',\n",
    "            shape=(self.attention_dim, input_shape[-1]),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Compute attention scores\n",
    "        attention_hidden = tf.nn.relu(\n",
    "            tf.matmul(inputs, self.attention_weights) + self.attention_bias\n",
    "        )\n",
    "        \n",
    "        attention_scores = tf.matmul(attention_hidden, self.output_weights)\n",
    "        \n",
    "        # Normalize to probabilities (sum to 1)\n",
    "        attention_scores = attention_scores - tf.reduce_max(attention_scores, axis=-1, keepdims=True)\n",
    "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        \n",
    "        # Apply attention: weight each feature by its importance\n",
    "        attended_features = inputs * attention_probs  \n",
    "        attended_features = tf.clip_by_value(attended_features, -5.0, 5.0)\n",
    "\n",
    "        # Store for visualization \n",
    "        self.last_attention_probs = tf.stop_gradient(attention_probs)\n",
    "        \n",
    "        return attended_features  \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'attention_dim': self.attention_dim})\n",
    "        return config\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Output shape is same as input shape\n",
    "        return input_shape\n",
    "\n",
    "print(\" FeatureAttentionLayer defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "980dba9b-b6d6-4578-9c6c-2939e5155463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING NEURAL NETWORK ARCHITECTURE\n",
      "\n",
      "Building model...\n",
      "\n",
      "Model architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"AttentionAccidentModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"AttentionAccidentModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ feature_attention               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,312</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FeatureAttentionLayer</span>)         │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_features (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ feature_attention               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m)             │         \u001b[38;5;34m5,312\u001b[0m │\n",
       "│ (\u001b[38;5;33mFeatureAttentionLayer\u001b[0m)         │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m10,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn1 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense2 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn2 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense3 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn3 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,139</span> (106.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,139\u001b[0m (106.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,691</span> (104.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,691\u001b[0m (104.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"BUILDING NEURAL NETWORK ARCHITECTURE\")\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "class FeatureAttentionLayer(layers.Layer):\n",
    "    def __init__(self, attention_dim=64, **kwargs):\n",
    "        super(FeatureAttentionLayer, self).__init__(**kwargs)\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_shape[-1], self.attention_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name='W_attention'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.attention_dim,),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name='b_attention'\n",
    "        )\n",
    "        self.u = self.add_weight(\n",
    "            shape=(self.attention_dim, 1),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name='u_attention'\n",
    "        )\n",
    "        super(FeatureAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Compute attention scores\n",
    "        v = tf.tanh(tf.matmul(inputs, self.W) + self.b)\n",
    "        vu = tf.matmul(v, self.u)\n",
    "        alphas = tf.nn.softmax(vu, axis=1)\n",
    "        # Weighted sum of inputs\n",
    "        output = inputs * alphas\n",
    "        return output\n",
    "\n",
    "# Build attention model\n",
    "def build_attention_model(input_dim, num_classes=3, attention_dim=64):\n",
    "    \n",
    "    # Input\n",
    "    inputs = keras.Input(shape=(input_dim,), name='input_features')\n",
    "    \n",
    "    # Attention\n",
    "    attended = FeatureAttentionLayer(attention_dim=attention_dim, name='feature_attention')(inputs)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(128, activation='relu', name='dense1')(attended)\n",
    "    x = layers.BatchNormalization(name='bn1')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout1')(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu', name='dense2')(x)\n",
    "    x = layers.BatchNormalization(name='bn2')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout2')(x)\n",
    "    \n",
    "    x = layers.Dense(32, activation='relu', name='dense3')(x)\n",
    "    x = layers.BatchNormalization(name='bn3')(x)\n",
    "    x = layers.Dropout(0.2, name='dropout3')(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='AttentionAccidentModel')\n",
    "    return model\n",
    "\n",
    "print(\"\\nBuilding model...\")\n",
    "model = build_attention_model(\n",
    "    input_dim=X_train_scaled.shape[1],\n",
    "    num_classes=3,\n",
    "    attention_dim=64\n",
    ")\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd62c1f3-9079-452e-b633-ed93b24c538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up training callbacks\n",
      "Callbacks configured\n"
     ]
    }
   ],
   "source": [
    "print(\"setting up training callbacks\")\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "import os\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "training_callbacks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        'models/best_attention_model.keras',  \n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6d8d345-08c9-469e-b317-c60f931d7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA VALIDATION CHECK\n",
      "Train NaN: 0, Train Inf: 0\n",
      "Test NaN:  0, Test Inf:  0\n",
      "Train min: -9.2053, max: 307.0920\n",
      "Train mean: -0.0000, std: 0.9938\n",
      "Extreme values clipped to [-10, 10]\n",
      "Train target unique: [0 1 2], range: [0, 2]\n",
      "Test target unique: [0 1 2]\n",
      "Data validation complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"DATA VALIDATION CHECK\")\n",
    "\n",
    "# 1. Check NaN / Inf\n",
    "train_nan = np.isnan(X_train_scaled).sum()\n",
    "test_nan = np.isnan(X_test_scaled).sum()\n",
    "train_inf = np.isinf(X_train_scaled).sum()\n",
    "test_inf = np.isinf(X_test_scaled).sum()\n",
    "\n",
    "print(f\"Train NaN: {train_nan}, Train Inf: {train_inf}\")\n",
    "print(f\"Test NaN:  {test_nan}, Test Inf:  {test_inf}\")\n",
    "\n",
    "if train_nan > 0 or train_inf > 0:\n",
    "    X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    print(\"Train data cleaned\")\n",
    "\n",
    "if test_nan > 0 or test_inf > 0:\n",
    "    X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    print(\"Test data cleaned\")\n",
    "\n",
    "# 2. Check data ranges\n",
    "print(f\"Train min: {X_train_scaled.min():.4f}, max: {X_train_scaled.max():.4f}\")\n",
    "print(f\"Train mean: {X_train_scaled.mean():.4f}, std: {X_train_scaled.std():.4f}\")\n",
    "\n",
    "# Clip extreme values\n",
    "if np.abs(X_train_scaled).max() > 10:\n",
    "    X_train_scaled = np.clip(X_train_scaled, -10, 10)\n",
    "    X_test_scaled = np.clip(X_test_scaled, -10, 10)\n",
    "    print(\"Extreme values clipped to [-10, 10]\")\n",
    "\n",
    "# 3. Check target variable\n",
    "print(f\"Train target unique: {np.unique(y_train)}, range: [{y_train.min()}, {y_train.max()}]\")\n",
    "print(f\"Test target unique: {np.unique(y_test)}\")\n",
    "\n",
    "print(\"Data validation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71d5b5da-ed86-4f93-a50e-22e6b2b67442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m582/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5791 - loss: 0.9298\n",
      "Epoch 1: val_accuracy improved from None to 0.74626, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6585 - loss: 0.7772 - val_accuracy: 0.7463 - val_loss: 0.6117 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7300 - loss: 0.6472\n",
      "Epoch 2: val_accuracy improved from 0.74626 to 0.75681, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7380 - loss: 0.6333 - val_accuracy: 0.7568 - val_loss: 0.5902 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m578/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7477 - loss: 0.6165\n",
      "Epoch 3: val_accuracy improved from 0.75681 to 0.76113, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.6115 - val_accuracy: 0.7611 - val_loss: 0.5792 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m561/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7547 - loss: 0.5998\n",
      "Epoch 4: val_accuracy improved from 0.76113 to 0.76590, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7556 - loss: 0.5993 - val_accuracy: 0.7659 - val_loss: 0.5721 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m585/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7580 - loss: 0.5931\n",
      "Epoch 5: val_accuracy improved from 0.76590 to 0.76770, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7589 - loss: 0.5908 - val_accuracy: 0.7677 - val_loss: 0.5658 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m581/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7618 - loss: 0.5835\n",
      "Epoch 6: val_accuracy improved from 0.76770 to 0.76855, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7607 - loss: 0.5854 - val_accuracy: 0.7685 - val_loss: 0.5612 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m588/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7626 - loss: 0.5820\n",
      "Epoch 7: val_accuracy improved from 0.76855 to 0.77181, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7634 - loss: 0.5790 - val_accuracy: 0.7718 - val_loss: 0.5587 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m571/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7638 - loss: 0.5765\n",
      "Epoch 8: val_accuracy did not improve from 0.77181\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7652 - loss: 0.5758 - val_accuracy: 0.7698 - val_loss: 0.5570 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m579/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7682 - loss: 0.5682\n",
      "Epoch 9: val_accuracy improved from 0.77181 to 0.77208, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7668 - loss: 0.5722 - val_accuracy: 0.7721 - val_loss: 0.5548 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m571/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7659 - loss: 0.5743\n",
      "Epoch 10: val_accuracy improved from 0.77208 to 0.77433, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7678 - loss: 0.5703 - val_accuracy: 0.7743 - val_loss: 0.5505 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m582/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7693 - loss: 0.5661\n",
      "Epoch 11: val_accuracy improved from 0.77433 to 0.77531, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7691 - loss: 0.5677 - val_accuracy: 0.7753 - val_loss: 0.5503 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7702 - loss: 0.5656\n",
      "Epoch 12: val_accuracy improved from 0.77531 to 0.77550, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7696 - loss: 0.5659 - val_accuracy: 0.7755 - val_loss: 0.5489 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m572/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.5624\n",
      "Epoch 13: val_accuracy did not improve from 0.77550\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7715 - loss: 0.5633 - val_accuracy: 0.7741 - val_loss: 0.5477 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m576/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.5611\n",
      "Epoch 14: val_accuracy did not improve from 0.77550\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.5610 - val_accuracy: 0.7751 - val_loss: 0.5455 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m570/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7743 - loss: 0.5592\n",
      "Epoch 15: val_accuracy improved from 0.77550 to 0.77679, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7729 - loss: 0.5597 - val_accuracy: 0.7768 - val_loss: 0.5441 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m567/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7729 - loss: 0.5590\n",
      "Epoch 16: val_accuracy improved from 0.77679 to 0.77703, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7728 - loss: 0.5585 - val_accuracy: 0.7770 - val_loss: 0.5430 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m562/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7736 - loss: 0.5591\n",
      "Epoch 17: val_accuracy did not improve from 0.77703\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7738 - loss: 0.5572 - val_accuracy: 0.7748 - val_loss: 0.5473 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m571/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7754 - loss: 0.5542\n",
      "Epoch 18: val_accuracy improved from 0.77703 to 0.77719, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7753 - loss: 0.5551 - val_accuracy: 0.7772 - val_loss: 0.5408 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m567/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7739 - loss: 0.5542\n",
      "Epoch 19: val_accuracy did not improve from 0.77719\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7740 - loss: 0.5546 - val_accuracy: 0.7772 - val_loss: 0.5412 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m577/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7771 - loss: 0.5505\n",
      "Epoch 20: val_accuracy did not improve from 0.77719\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7747 - loss: 0.5540 - val_accuracy: 0.7761 - val_loss: 0.5411 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m580/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7758 - loss: 0.5513\n",
      "Epoch 21: val_accuracy improved from 0.77719 to 0.77852, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7743 - loss: 0.5540 - val_accuracy: 0.7785 - val_loss: 0.5386 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m583/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7746 - loss: 0.5528\n",
      "Epoch 22: val_accuracy improved from 0.77852 to 0.77891, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7757 - loss: 0.5512 - val_accuracy: 0.7789 - val_loss: 0.5383 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m569/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7751 - loss: 0.5512\n",
      "Epoch 23: val_accuracy improved from 0.77891 to 0.77918, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7756 - loss: 0.5510 - val_accuracy: 0.7792 - val_loss: 0.5385 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m562/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7758 - loss: 0.5497\n",
      "Epoch 24: val_accuracy improved from 0.77918 to 0.77934, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7758 - loss: 0.5496 - val_accuracy: 0.7793 - val_loss: 0.5366 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m580/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7770 - loss: 0.5469\n",
      "Epoch 25: val_accuracy did not improve from 0.77934\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7766 - loss: 0.5492 - val_accuracy: 0.7791 - val_loss: 0.5382 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m561/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7759 - loss: 0.5473\n",
      "Epoch 26: val_accuracy improved from 0.77934 to 0.77955, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7758 - loss: 0.5487 - val_accuracy: 0.7796 - val_loss: 0.5370 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m585/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7789 - loss: 0.5461\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.77955\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7777 - loss: 0.5468 - val_accuracy: 0.7785 - val_loss: 0.5380 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m571/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7825 - loss: 0.5374\n",
      "Epoch 28: val_accuracy improved from 0.77955 to 0.77976, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7796 - loss: 0.5437 - val_accuracy: 0.7798 - val_loss: 0.5333 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m587/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7786 - loss: 0.5427\n",
      "Epoch 29: val_accuracy improved from 0.77976 to 0.78024, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7792 - loss: 0.5417 - val_accuracy: 0.7802 - val_loss: 0.5330 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m565/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7801 - loss: 0.5408\n",
      "Epoch 30: val_accuracy improved from 0.78024 to 0.78032, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7790 - loss: 0.5420 - val_accuracy: 0.7803 - val_loss: 0.5319 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m569/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7780 - loss: 0.5421\n",
      "Epoch 31: val_accuracy improved from 0.78032 to 0.78117, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7790 - loss: 0.5408 - val_accuracy: 0.7812 - val_loss: 0.5313 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m566/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7801 - loss: 0.5403\n",
      "Epoch 32: val_accuracy improved from 0.78117 to 0.78167, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7797 - loss: 0.5401 - val_accuracy: 0.7817 - val_loss: 0.5306 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m563/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7825 - loss: 0.5378\n",
      "Epoch 33: val_accuracy did not improve from 0.78167\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7803 - loss: 0.5393 - val_accuracy: 0.7813 - val_loss: 0.5308 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m563/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7803 - loss: 0.5379\n",
      "Epoch 34: val_accuracy improved from 0.78167 to 0.78188, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7799 - loss: 0.5390 - val_accuracy: 0.7819 - val_loss: 0.5308 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m564/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7824 - loss: 0.5375\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.78188\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7810 - loss: 0.5376 - val_accuracy: 0.7806 - val_loss: 0.5314 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m565/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7809 - loss: 0.5354\n",
      "Epoch 36: val_accuracy improved from 0.78188 to 0.78220, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7809 - loss: 0.5370 - val_accuracy: 0.7822 - val_loss: 0.5286 - learning_rate: 2.5000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m567/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7826 - loss: 0.5357\n",
      "Epoch 37: val_accuracy improved from 0.78220 to 0.78300, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7832 - loss: 0.5344 - val_accuracy: 0.7830 - val_loss: 0.5285 - learning_rate: 2.5000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m564/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7833 - loss: 0.5323\n",
      "Epoch 38: val_accuracy did not improve from 0.78300\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7827 - loss: 0.5333 - val_accuracy: 0.7820 - val_loss: 0.5276 - learning_rate: 2.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m565/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7825 - loss: 0.5327\n",
      "Epoch 39: val_accuracy did not improve from 0.78300\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7828 - loss: 0.5338 - val_accuracy: 0.7825 - val_loss: 0.5276 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m577/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7826 - loss: 0.5342\n",
      "Epoch 40: val_accuracy improved from 0.78300 to 0.78329, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7822 - loss: 0.5341 - val_accuracy: 0.7833 - val_loss: 0.5275 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m588/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7822 - loss: 0.5341\n",
      "Epoch 41: val_accuracy did not improve from 0.78329\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7832 - loss: 0.5333 - val_accuracy: 0.7827 - val_loss: 0.5275 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m586/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7825 - loss: 0.5336\n",
      "Epoch 42: val_accuracy did not improve from 0.78329\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7828 - loss: 0.5332 - val_accuracy: 0.7829 - val_loss: 0.5276 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m562/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7829 - loss: 0.5320\n",
      "Epoch 43: val_accuracy did not improve from 0.78329\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7826 - loss: 0.5320 - val_accuracy: 0.7830 - val_loss: 0.5272 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m561/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7844 - loss: 0.5292\n",
      "Epoch 44: val_accuracy did not improve from 0.78329\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7840 - loss: 0.5313 - val_accuracy: 0.7833 - val_loss: 0.5275 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m568/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7845 - loss: 0.5283\n",
      "Epoch 45: val_accuracy did not improve from 0.78329\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7831 - loss: 0.5320 - val_accuracy: 0.7831 - val_loss: 0.5268 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m564/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7845 - loss: 0.5312\n",
      "Epoch 46: val_accuracy improved from 0.78329 to 0.78374, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7835 - loss: 0.5329 - val_accuracy: 0.7837 - val_loss: 0.5269 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m588/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7832 - loss: 0.5325\n",
      "Epoch 47: val_accuracy did not improve from 0.78374\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7844 - loss: 0.5319 - val_accuracy: 0.7831 - val_loss: 0.5272 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m587/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7850 - loss: 0.5304\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.78374\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7844 - loss: 0.5317 - val_accuracy: 0.7832 - val_loss: 0.5268 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m585/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7844 - loss: 0.5309\n",
      "Epoch 49: val_accuracy improved from 0.78374 to 0.78377, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7838 - loss: 0.5310 - val_accuracy: 0.7838 - val_loss: 0.5262 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m567/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7853 - loss: 0.5283\n",
      "Epoch 50: val_accuracy improved from 0.78377 to 0.78385, saving model to models/best_attention_model.keras\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7845 - loss: 0.5293 - val_accuracy: 0.7838 - val_loss: 0.5257 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 50.\n"
     ]
    }
   ],
   "source": [
    "model = build_attention_model(\n",
    "    input_dim=X_train_scaled.shape[1],\n",
    "    num_classes=3,\n",
    "    attention_dim=64\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    callbacks=training_callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cfcea725-682f-4260-91f0-524c71327df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1658c589-0ca1-470b-b388-baadbe0ff36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Feature  Attention_Weight\n",
      "16                    Stop          0.136232\n",
      "2              Humidity(%)          0.104751\n",
      "35     Wind_Direction_Calm          0.087900\n",
      "51    Wind_Direction_South          0.080873\n",
      "70      Weather_Clean_Rain          0.074341\n",
      "42      Wind_Direction_NNE          0.059703\n",
      "41       Wind_Direction_NE          0.058721\n",
      "46        Wind_Direction_S          0.048469\n",
      "34          cell1_mean_sev          0.048229\n",
      "15                 Station          0.045527\n",
      "60  Sunrise_Sunset_Unknown          0.033037\n",
      "45    Wind_Direction_North          0.030213\n",
      "61    Weather_Clean_Cloudy          0.030132\n",
      "3             Pressure(in)          0.028501\n",
      "43      Wind_Direction_NNW          0.026514\n",
      "38      Wind_Direction_ESE          0.025654\n",
      "57      Wind_Direction_WSW          0.022289\n",
      "1            Wind_Chill(F)          0.021082\n",
      "11                Junction          0.019924\n",
      "58     Wind_Direction_West          0.019304\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Get attention layer\n",
    "attention_layer = model.get_layer(\"feature_attention\")\n",
    "\n",
    "# Define a new model to output the attention scores\n",
    "attention_model = tf.keras.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=attention_layer.output  # this will return the attention scores\n",
    ")\n",
    "\n",
    "# Run prediction to get attention scores\n",
    "attention_scores = attention_model.predict(X_test_scaled[:500], verbose=0)\n",
    "\n",
    "# Average over batch and queries to get feature importance\n",
    "mean_attention = attention_scores.mean(axis=0)  # adjust axis depending on shape\n",
    "mean_attention = mean_attention.mean(axis=0) if mean_attention.ndim > 1 else mean_attention\n",
    "\n",
    "# Build dataframe\n",
    "att_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Attention_Weight': mean_attention\n",
    "}).sort_values('Attention_Weight', ascending=False)\n",
    "\n",
    "print(att_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad0e5cb8-d912-4cda-8465-d8e4a0e8c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL EVALUATION\n",
      "\n",
      "1. Making predictions on test set...\n",
      "\n",
      "OVERALL METRICS:\n",
      "   Accuracy:  0.7864\n",
      "   Precision: 0.7766 (macro)\n",
      "   Recall:    0.7446 (macro)\n",
      "   F1-Score:  0.7543 (macro)\n",
      "\n",
      "PER-CLASS METRICS:\n",
      "\n",
      "   Minor:\n",
      "      Precision: 0.7933\n",
      "      Recall:    0.7805\n",
      "      F1-Score:  0.7868\n",
      "      Support:   20,209\n",
      "\n",
      "   Moderate:\n",
      "      Precision: 0.7374\n",
      "      Recall:    0.5414\n",
      "      F1-Score:  0.6244\n",
      "      Support:   20,209\n",
      "\n",
      "   Severe:\n",
      "      Precision: 0.7991\n",
      "      Recall:    0.9118\n",
      "      F1-Score:  0.8518\n",
      "      Support:   40,417\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "   (Rows = True, Columns = Predicted)\n",
      "\n",
      "                     Minor   Moderate     Severe\n",
      "   Minor            15,773      1,683      2,753\n",
      "   Moderate          2,758     10,942      6,509\n",
      "   Severe            1,353      2,213     36,851\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "print(\"MODEL EVALUATION\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n1. Making predictions on test set...\")\n",
    "y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "# Handle multi-class or binary\n",
    "if y_pred_proba.shape[1] > 1:\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "else:\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\nOVERALL METRICS:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   Precision: {precision_macro:.4f} (macro)\")\n",
    "print(f\"   Recall:    {recall_macro:.4f} (macro)\")\n",
    "print(f\"   F1-Score:  {f1_macro:.4f} (macro)\")\n",
    "\n",
    "print(\"\\nPER-CLASS METRICS:\")\n",
    "class_names = ['Minor', 'Moderate', 'Severe']\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"\\n   {name}:\")\n",
    "    print(f\"      Precision: {precision[i]:.4f}\")\n",
    "    print(f\"      Recall:    {recall[i]:.4f}\")\n",
    "    print(f\"      F1-Score:  {f1[i]:.4f}\")\n",
    "    print(f\"      Support:   {support[i]:,}\")\n",
    "\n",
    "print(\"\\nCONFUSION MATRIX:\")\n",
    "print(\"   (Rows = True, Columns = Predicted)\")\n",
    "print(f\"\\n   {'':12} {'Minor':>10} {'Moderate':>10} {'Severe':>10}\")\n",
    "for i, name in enumerate(class_names):\n",
    "    row = f\"   {name:12}\"\n",
    "    for j in range(3):\n",
    "        row += f\" {cm[i,j]:10,}\"\n",
    "    print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
